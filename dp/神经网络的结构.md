（mnist 数据集）
为什么分层，变种非常多，最经典的原版是 MLP 多层感知机
![[截屏2026-01-19 15.01.04.png]]
 神经网络 neural network模仿大脑结构
 神经元可以理解成一个用来装 0 到 1数字的容器，这个数叫做“激活值”activation
![[截屏2026-01-19 15.16.10.png]]
网络最后一层的数值也就是激活值同理都是 0 到 1 之间，这些值的大小表示系统认为输入的图像对应着哪个数字的可能性
🧚‍♀️这只是全联接层吧，不包括前面的卷积和池化
![[截屏2026-01-19 15.19.06.png]] 隐藏层可以看成一个里面进行处理识别数字的具体工作的大黑箱，这里选择两层，每层有 16 个神经元（16 个是为了显得好看），实际应用中网络结构有很大的调整实验的余地
神经网络运作时上层的激活值将决定下层的激活值
神经网络处理信息的**核心机制**是研究一层的激活值是通过怎样的运算来算出下一层的激活值的，某种程度上说神经网络是想模仿生物钟神经元组成的网络，某些神经元的激发会促使另一些神经元激发![[截屏2026-01-19 16.09.53.png]]
输入层 $28\times 28$ 像素点的激活值（即输入图像各像素的灰度值）的图案会让下层的激活值产生某些特殊的图案（模式），这层激活值的图案（模式）会让下层的激活值产生某些特殊的图案（模式），再让下面一层产生特殊的图案（模式），最终在输出层得到某种结果，输出层最亮的神经元就是神经网络的选择
🧚‍♀️玄学隐含层数, 玄学神经元个数；汉字的话可以将中间层设为对偏旁的检测对上部的检测等等；老铁汉字就用卷积网络了；可以看看残差网络和 google inception 怎么增加深度；gpu 核数是 2 的次方，16 这类计算更有效率；无论第一层怎么排列，都不影响与下一层如何交互；还是概率然后什么算法能提高概率，增加准确度？这就是神经元学习的关键；回前面：神经元最厉害的就是感知，不需要那么多数学运算过程，就是联通什么就能导向默认结果，所以不会累

❓网络每层间如何影响训练过程的数学原理是？
❓为什么我们觉得这种层状结构可以做到智能判断？期望中间层做什么呢
==>理想情况下希望倒数第二层的各个神经元能够分别对应上一个笔画部件![[截屏2026-01-20 09.01.01.png]]
第三层到最后一层只需要学习哪些部件能组合出哪个数字即可
🧚‍♀️讲解只是打个比方，并不是拆分法，特征是可以多种多样的； gpu 在做计算需要大量的算法；神经网络的隐藏层（hidden）本质上就是对我们难以归类的特征进行提取；这都是 gpu 在做计算需要大量的算法；人脑又不是按像素点处理的。。。；不同的神经元处理不同的信息，而且真正处理的时候不也是转换成电信号么；讲解只是打个比方，并不是拆分法，特征是可以多种多样的；但实际情况是，隐藏层根本不知道对应的啥；本质上是猜，猜的过程涉及各种数学优化，比如牛顿法，梯度下降，结果则用概率论描述；样本数据–>特征提取–>训练函数–>模式识别；每一次分解子问题就是一个神经层；Edge detection，卷积的作用就是提取 edge 特征；这段讲的其实是告诉我们，中间层是负责输出特征的；可以把这个网络倒过来看，就变成了把一个数字进行一步步拆分；神经网络隐藏层的含义一般都不知道的人只是设定数量这里 up 告诉大家他的原理而不是真正含义；上层是底层的组合；思路是一样的，把离散的特征逐渐聚合，形成更明显，更易识别的特征；这是个启发式的讲解，实际上背后是如何用高维函数确定识别面的问题；这里大家可能会陷入误区，想要手动调参![[截屏2026-01-20 09.16.05.png]]
除了图像识别，世界上各种人工智能任务都可以分解成抽象元素一层层抽丝剥茧，要设计上一层的激活值如何决定下一层中的激活值

🧚‍♀️压缩到很小比例的可识别特征；？你不觉得语音识别是对声音的语谱图做目标检测吗？这俩不是一个东西吗？；连线就是权值，z=aw+b，一次函数；音频信号的信息少，噪声还多；突然想到了最基本的电路，连续性以及未来人工智能的连续性；第二层中的一个 neural，用于检测 input 中某一特定区域的 pattern；这是特征值，矩阵降维，这里只是一种想法，让大家好理解
