（mnist 数据集）
为什么分层，变种非常多，最经典的原版是 MLP 多层感知机
![[截屏2026-01-19 15.01.04.png]]
 神经网络 neural network模仿大脑结构
 神经元可以理解成一个用来装 0 到 1数字的容器，这个数叫做“激活值”activation
![[截屏2026-01-19 15.16.10.png]]
网络最后一层的数值也就是激活值同理都是 0 到 1 之间，这些值的大小表示系统认为输入的图像对应着哪个数字的可能性
🧚‍♀️这只是全联接层吧，不包括前面的卷积和池化
![[截屏2026-01-19 15.19.06.png]] 隐藏层可以看成一个里面进行处理识别数字的具体工作的大黑箱，这里选择两层，每层有 16 个神经元（16 个是为了显得好看），实际应用中网络结构有很大的调整实验的余地
神经网络运作时上层的激活值将决定下层的激活值
神经网络处理信息的**核心机制**是研究一层的激活值是通过怎样的运算来算出下一层的激活值的，某种程度上说神经网络是想模仿生物钟神经元组成的网络，某些神经元的激发会促使另一些神经元激发![[截屏2026-01-19 16.09.53.png]]
输入层 $28\times 28$ 像素点的激活值（即输入图像各像素的灰度值）的图案会让下层的激活值产生某些特殊的图案（模式），这层激活值的图案（模式）会让下层的激活值产生某些特殊的图案（模式），再让下面一层产生特殊的图案（模式），最终在输出层得到某种结果，输出层最亮的神经元就是神经网络的选择
🧚‍♀️玄学隐含层数, 玄学神经元个数；汉字的话可以将中间层设为对偏旁的检测对上部的检测等等；老铁汉字就用卷积网络了；可以看看残差网络和 google inception 怎么增加深度；gpu 核数是 2 的次方，16 这类计算更有效率；无论第一层怎么排列，都不影响与下一层如何交互；还是概率然后什么算法能提高概率，增加准确度？这就是神经元学习的关键；回前面：神经元最厉害的就是感知，不需要那么多数学运算过程，就是联通什么就能导向默认结果，所以不会累

❓网络每层间如何影响训练过程的数学原理是？
❓为什么我们觉得这种层状结构可以做到智能判断？期望中间层做什么呢
==>理想情况下希望倒数第二层的各个神经元能够分别对应上一个笔画部件![[截屏2026-01-20 09.01.01.png]]
第三层到最后一层只需要学习哪些部件能组合出哪个数字即可
🧚‍♀️讲解只是打个比方，并不是拆分法，特征是可以多种多样的； gpu 在做计算需要大量的算法；神经网络的隐藏层（hidden）本质上就是对我们难以归类的特征进行提取；这都是 gpu 在做计算需要大量的算法；人脑又不是按像素点处理的。。。；不同的神经元处理不同的信息，而且真正处理的时候不也是转换成电信号么；讲解只是打个比方，并不是拆分法，特征是可以多种多样的；但实际情况是，隐藏层根本不知道对应的啥；本质上是猜，猜的过程涉及各种数学优化，比如牛顿法，梯度下降，结果则用概率论描述；样本数据–>特征提取–>训练函数–>模式识别；每一次分解子问题就是一个神经层；Edge detection，卷积的作用就是提取 edge 特征；这段讲的其实是告诉我们，中间层是负责输出特征的；可以把这个网络倒过来看，就变成了把一个数字进行一步步拆分；神经网络隐藏层的含义一般都不知道的人只是设定数量这里 up 告诉大家他的原理而不是真正含义；上层是底层的组合；思路是一样的，把离散的特征逐渐聚合，形成更明显，更易识别的特征；这是个启发式的讲解，实际上背后是如何用高维函数确定识别面的问题；这里大家可能会陷入误区，想要手动调参![[截屏2026-01-20 09.16.05.png]]
除了图像识别，世界上各种人工智能任务都可以分解成抽象元素一层层抽丝剥茧，要设计上一层的激活值如何决定下一层中的激活值

🧚‍♀️压缩到很小比例的可识别特征；？你不觉得语音识别是对声音的语谱图做目标检测吗？这俩不是一个东西吗？；连线就是权值，z=aw+b，一次函数；音频信号的信息少，噪声还多；突然想到了最基本的电路，连续性以及未来人工智能的连续性；第二层中的一个 neural，用于检测 input 中某一特定区域的 pattern；这是特征值，矩阵降维，这里只是一种想法，让大家好理解
![[截屏2026-01-20 09.26.40.png]] 第二层的一个神经元和所有第一层的神经元之间需要设计权重，将权重值看成一个表格会更好理解，如果将关注区域的权重赋为正值，而将其他所有的权重值赋为 0，这样对所有的像素值取加权和就只会累加我们关注区域的像素值了
神经元中的激活值实际上是一个**对加权和到底多正**的打分![[截屏2026-01-20 09.45.43.png]]
但是有时候即使加权和大于 0 也不想将神经元点亮，例如只有和大于 10 才让他激发，此时就需要加一个偏置值 bias保证不能随便激发（🧚‍♀️高通滤波器？）
Weight告知第二层的神经元关注什么样的像素图案，bias 告知加权和得多大（🧚‍♀️就是 intercept 截距），所以一个神经元带有 784 个权重，第二层的每一个神经元都会和第一层全部的 784 个神经元相连接且每一个神经元的 784 接线上还会带着一个权重，每一个神经元都会在计算自己的加权和后加上自己的偏置，之后再通过 sigmoid 压缩输出自己的结果 ![[Pasted image 20260120095501.png]] 
🧚‍♀️第二列的数字就是被卷积核扫描区域图片的像素值，而权重就是卷积核中的数字！；二维码只是简单的 0 与 1，这个是灰度值或者说权重，不是只有 0 和 1；不出所料是线性组合……；权重（Weight） 是指在模型中用于调整输入特征对输出的影响程度的参数。权重反映了每个特征在模型中的相对重要性；存在激活函数，所以是多层非线性；线性就没必要分层了；所以向量就是在这里用的是吧？；高等代数的线性相关性在这里出现，震惊；最简单的当然是线性的但问题一般来说总不是的；忠告啊这里要有 w 0；这里权重值怎么来的，为啥有正有负；在反向传播出来之前，多层是难以计算的；这能识别特定位置的一条线, 谁能告诉我, 在其他位置的横线怎么识别?；如果你只在你想要的区域把权值取正值，则无论你前面的神经元值是多少，最后映射的图像一定是你想要的那个图像，因此要在想要区域的上下取负值；如果只有正权重，当区域内有线的时候会得到很大的值，换言之，只有横线在合适的宽度及位置的时候，检测矩阵才能到达理论最大值，如果完全吻合，加没加负权重的最大值都是一样的，理论值为 $1 \times 像素个数$，如果目标横线比检测矩阵偏上或偏下在上下的负权重会减少最终值 ;负值就相等于一个惩罚因子，惩罚哪些不是 0 的; 我不理解，激活函数不是引入非线性因素吗？为什么这里的作用看起来像在正则化 ;我反应过来了，正则化是防止过拟合，为了减少泛化误差的; 我的第一反应是反正切....；<=这个就是平移+放缩后的双曲正切; tanh 确实也是其中一种；简单粗暴的一刀切线性函数反而是最好的；logistic 回归这部分去看吴恩达的视频挺好的；还可以让像素更多利用排除法以及常见的字体见部识总；一个神经元提取了一组特征（一组权重参数），多个神经元，就提取多组特征（就有多少组权重参数）；没事的，学 c 语言的第三节课就是展示计算机的计算速度：每秒几百万次轻轻松松；对于图像来说全连接计算量太大，所以基本都是用卷积核来做；这计算量相比于后面反向传播调参的消耗而言，真的不值一提；阀值瞬间让线性变成了非线性；这是深度神经网 DNN 比较节省空间的还有卷积神经网 CNN 和递归神经网 RNN
