（mnist 数据集）
为什么分层，变种非常多，最经典的原版是 MLP 多层感知机
![[截屏2026-01-19 15.01.04.png]]
 神经网络 neural network模仿大脑结构
 神经元可以理解成一个用来装 0 到 1数字的容器，这个数叫做“激活值”activation
![[截屏2026-01-19 15.16.10.png]]
网络最后一层的数值也就是激活值同理都是 0 到 1 之间，这些值的大小表示系统认为输入的图像对应着哪个数字的可能性
🧚‍♀️这只是全联接层吧，不包括前面的卷积和池化
![[截屏2026-01-19 15.19.06.png]] 隐藏层可以看成一个里面进行处理识别数字的具体工作的大黑箱，这里选择两层，每层有 16 个神经元（16 个是为了显得好看），实际应用中网络结构有很大的调整实验的余地
神经网络运作时上层的激活值将决定下层的激活值
神经网络处理信息的**核心机制**是研究一层的激活值是通过怎样的运算来算出下一层的激活值的，某种程度上说神经网络是想模仿生物钟神经元组成的网络，某些神经元的激发会促使另一些神经元激发![[截屏2026-01-19 16.09.53.png]]
输入层 $28\times 28$ 像素点的激活值（即输入图像各像素的灰度值）的图案会让下层的激活值产生某些特殊的图案（模式），这层激活值的图案（模式）会让下层的激活值产生某些特殊的图案（模式），再让下面一层产生特殊的图案（模式），最终在输出层得到某种结果，输出层最亮的神经元就是神经网络的选择
🧚‍♀️玄学隐含层数, 玄学神经元个数；汉字的话可以将中间层设为对偏旁的检测对上部的检测等等；老铁汉字就用卷积网络了；可以看看残差网络和 google inception 怎么增加深度；gpu 核数是 2 的次方，16 这类计算更有效率；无论第一层怎么排列，都不影响与下一层如何交互；还是概率然后什么算法能提高概率，增加准确度？这就是神经元学习的关键；回前面：神经元最厉害的就是感知，不需要那么多数学运算过程，就是联通什么就能导向默认结果，所以不会累

❓网络每层间如何影响训练过程的数学原理是？
❓为什么我们觉得这种层状结构可以做到智能判断？期望中间层做什么呢
==>理想情况下希望倒数第二层的各个神经元能够分别对应上一个笔画部件![[截屏2026-01-20 09.01.01.png]]
第三层到最后一层只需要学习哪些部件能组合出哪个数字即可
🧚‍♀️讲解只是打个比方，并不是拆分法，特征是可以多种多样的； gpu 在做计算需要大量的算法；神经网络的隐藏层（hidden）本质上就是对我们难以归类的特征进行提取；这都是 gpu 在做计算需要大量的算法；人脑又不是按像素点处理的。。。；不同的神经元处理不同的信息，而且真正处理的时候不也是转换成电信号么；讲解只是打个比方，并不是拆分法，特征是可以多种多样的；但实际情况是，隐藏层根本不知道对应的啥