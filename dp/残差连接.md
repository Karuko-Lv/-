首先，我们来看看 **残差连接 (Residual Connection)**。
![[Gemini_Generated_Image_ueftlyueftlyueft.png]]
就像我在图上指的一样，残差连接就像一条“捷径”。它允许信息绕过一些层，直接传递到后面的层。这有助于解决深层网络训练中的“梯度消失”问题，让网络更容易学习到复杂的特征，就像在图的右侧思想气泡里展示的公式 `x + f(x)`。
接下来是 **层归一化 (Layer Normalization)**。

如上图所示，层归一化通常紧跟在残差连接之后。它的作用就像一个“整理器”，它对这一层所有神经元的输出进行标准化，使它们的分布更加稳定。这有助于加速训练过程，并提高模型的泛化能力。

最后，我们来了解 **前馈网络 (Feedforward Network)**。
