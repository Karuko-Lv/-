在 AI 领域，有两个名字很像、但性质完全不同的“参数”。

训练模型的过程，其实是**机器调一种参数**，而**人调另一种参数**的过程。

我们需要区分这两个概念：**模型参数 (Model Parameters)** 和 **超参数 (Hyperparameters)**。

---

### 1. 机器调的参数：模型参数 (Model Parameters)

**这就是通常所说的“训练 (Training)”。**

- **这是什么？** 就是我们之前聊到的 **权重 ($w$, Weights)** 和 **偏置 ($b$, Biases)**。
    
- **谁来调？** **完全由计算机自动完成。** 人类无法直接插手。
    
- **怎么调？** 通过**反向传播**和**梯度下降**。机器每看一批数据，算出误差，就自动微调一下这几十亿个参数。
    
- **数量级：** 极其庞大。比如 GPT-3 有 1750 亿个这种参数。
    
- **本质：** 这是模型学到的“知识”本身。
    

> **一句话总结：** 训练就是机器在疯狂地做“微积分题”，自动把这 1750 亿个开关拨到正确的位置。

---

### 2. 人调的参数：超参数 (Hyperparameters)

**这就是通常所说的“调参 (Tuning)”或者戏称的“炼丹”。**

- **这是什么？** 这是在训练开始**之前**，人为设定的“配置项”。它们决定了模型“怎么学”、“学多快”、“长什么样”。
    
- **谁来调？** **由 AI 工程师（炼丹师）手动设置**，或者写个脚本去搜索。
    
- **怎么调？** 靠经验、直觉、或者暴力试错（Grid Search）。
    
- **数量级：** 比较少，通常几十个。
    
- **常见例子：**
    
    - **学习率 (Learning Rate):** 步子迈多大？（太大容易扯着蛋，太小走得慢）。
        
    - **Batch Size:** 一次背多少个单词？
        
    - **层数 (Layers):** 盖几层楼？
        
    - **隐藏单元数:** 每一层请几个侦探？
        

> **一句话总结：** 调参就是你在旁边当“教练”，制定训练计划（比如：“今天跑5公里，明天休息”），但真正跑步（训练）的是模型自己。

---

### 3. 一个生动的比喻：烤面包

想象你在用烤箱**烤面包**（训练模型）：

- **调参 (Tuning Hyperparameters):**
    
    - 这是**你**做的事。
        
    - 你设定烤箱温度是 200度还是 180度？（学习率）
        
    - 你设定烤多久？30分钟还是40分钟？（训练轮数 Epochs）
        
    - 你决定放多少面粉、多少糖？（网络结构设计）
        
    - _一旦你按下“开始”键，你的工作就暂时结束了。_
        
- **训练 (Training Model Parameters):**
    
    - 这是**烤箱内部**发生的事。
        
    - 面粉里的分子结构发生化学反应（权重 $w$ 在变化）。
        
    - 面团膨胀变色（Loss 在下降）。
        
    - _这个过程是根据物理定律（梯度下降）自动发生的，你没法钻进烤箱去捏每一个面粉分子。_
        

---

### 4. 总结对比表

|**特性**|**模型参数 (Model Parameters)**|**超参数 (Hyperparameters)**|
|---|---|---|
|**通俗叫法**|**“权重”**|**“配置” / “配方”**|
|**对应动作**|**训练 (Training)**|**调参 (Tuning)**|
|**谁来干**|算法/计算机自动干|工程师/研究员手动干|
|**什么时候干**|训练过程中实时更新|训练开始前设定好|
|**目的是什么**|最小化误差 (Loss)|寻找让训练最高效的配置|
|**例子**|神经网络连接线的数值 ($w$)|学习率、层数、Batch size|

**所以，回到你的问题：**

当你听到**“训练模型”时，指的是计算机在辛苦地算那几百亿个 $w$。

当你听到“那个模型效果不好，我去调个参”时，指的是工程师去修改学习率或网络结构，然后让计算机重新训练**一遍。