 Back propagation
计算梯度的算法 BP

❓神经网络怎么样前馈信息的？==>**加权、激活、传递**
学习就是找到特定的权重偏置使得一个代价函数最小化 

Loss 函数告诉如何改变所有连线上的权重偏置，好让代价下降最快
梯度向量每一项的大小是代价函数对每个参数敏感程度的刻画
🧚‍♀️严格来说，改变量应为无穷小（至少足够小）；注意，只是在这个点处的梯度；不同的神经元不是有不同的权重数组吗，图形上一组梯度不是应该对应同个神经元的不同权重吗


代价函数涉及到成千上万个训练样本的代价取平均值，原则上调整每一步梯度下降用的权重偏置也会基于所有的训练样本
为了计算效率（不希望每一步都得计算所有训练样本）
❓❓❓❓❓单个训练数据会具体让每个权重和每个偏置产生怎样的变化 
如果网络没有被完全训练好，输出层的激活值就会看起来很随机，首先不能直接改动激活值，只能改变权重和偏置值，但是得记住最终的目标（输出层的变动），变动的大小应该和现在值与目标值之间的差呈正比
![[截屏2026-01-20 16.29.47.png]] 🧚‍♀️小批量；只改权重的化还会有相同的；那样会导致欠拟合；