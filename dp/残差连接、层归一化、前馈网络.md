## **残差连接 (Residual Connection)**
![[Gemini_Generated_Image_ueftlyueftlyueft.png]]
就像在图上指的一样，残差连接就像一条“捷径”。它允许信息绕过一些层，直接传递到后面的层。这有助于解决深层网络训练中的“梯度消失”问题，让网络更容易学习到复杂的特征
~={red}==>原理：公式 $x + f(x)$ =~

---
## **层归一化 (Layer Normalization)**
![[Gemini_Generated_Image_ueftlyueftlyueft (1).png]]
如上图所示，**层归一化通常紧跟在残差连接之后**。它的作用就像一个“整理器”，对这一层所有神经元的输出进行标准化，使它们的分布更加稳定。这有助于加速训练过程，并提高模型的泛化能力。
### 1. 什么是“分布不稳定”？
假设：一个神经网络的一层有 4 个神经元。在没有任何限制的情况下，它们的输出值可能会因为权重初始化或者输入的变化而变得非常极端。

🧐PS：权重初始化需要“不多也不少”，即**保持每一层输出的方差与输入的方差基本一致**。现代主流的初始化方法如下
#### 现代主流的初始化方法

为了实现这个平衡，科学家们提出了两种最常用的公式：
##### A . Xavier 初始化 (Glorot Initialization)
适用场景： 激活函数是 Sigmoid 或 Tanh 时
核心思想：权重应该根据输入和输出神经元的数量来缩放。$W \sim U\left(-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}}\right)$
- $n_{in}$：输入神经元个数
- $n_{out}$：输出神经元个数
##### B. He 初始化 (Kaiming Initialization)

适用场景： 激活函数是 ReLU（现在最常用的）时
核心思想：因为 ReLU 会把一半的输入变成 0，所以需要更强的初始权重来补偿。$W \sim N\left(0, \sqrt{\frac{2}{n_{in}}}\right)$
例：原始输出数据
假设某一层的输出向量是：
$$X = [10, 2, -5, 25]$$
- **问题所在：** 这些数字跨度很大（从 -5 到 25）。
- **后果：** 如果下一层有一个激活函数（比如 Sigmoid），25 会让神经元直接“饱和”（输出变成 1，梯度几乎为 0），而 -5 也会让它饱和（输出变成 0）。这会导致模型很难学习，也就是所谓的**梯度消失**。
### 2. 具体的标准化计算步骤
🌟“使分布稳定”通常是指把这些数字转化为**平均值为 0，方差为 1** 的一组新数字。
我们对 $X = [10, 2, -5, 25]$ 进行计算：
1. 计算平均值 (Mean):$$(10 + 2 - 5 + 25) / 4 = 32 / 4 = 8$$
2. 计算方差 (Variance):
    计算每个数与平均值的差的平方，再取平均：$$[(10-8)^2 + (2-8)^2 + (-5-8)^2 + (25-8)^2] / 4$$$$= [4 + 36 + 169 + 289] / 4 = 498 / 4 = 124.5$$
3. 计算标准差 (Standard Deviation):
    $$\sqrt{124.5} \approx 11.16$$
4. 标准化处理 (Standardize):
    对每个数字执行：$\text{新值} = (\text{旧值} - \text{平均值}) / \text{标准差}$

#### 3. 标准化后的数据（稳定的分布）

经过上面的计算，原来的向量变成了：
- $10 \rightarrow (10 - 8) / 11.16 \approx \mathbf{0.18}$
- $2 \rightarrow (2 - 8) / 11.16 \approx \mathbf{-0.54}$
- $-5 \rightarrow (-5 - 8) / 11.16 \approx \mathbf{-1.16}$
- $25 \rightarrow (25 - 8) / 11.16 \approx \mathbf{1.52}$
**最终输出：$[0.18, -0.54, -1.16, 1.52]$**
#### 4. 为什么这叫“更加稳定”？

对比一下变化：

- **原始数据：** $[10, 2, -5, 25]$（波动巨大，最大值是最小值的几十倍）
    
- **归一化后：** $[0.18, -0.54, -1.16, 1.52]$（数据被压缩到了一个很小的、以 0 为中心的范围内）
    

**这种“稳定”带来的好处：**

1. **不挑剔初始化：** 不管原来的数字是 $[100, 200...]$ 还是 $[0.1, 0.2...]$，经过处理后都会变成类似的范围。
    
2. **加速收敛：** 所有的输入都在激活函数反应最灵敏的区域（比如 0 附近），梯度很大，模型学得飞快。
    
3. **防止爆炸：** 它像一个“减压阀”，防止神经网络里的数字在层层传递中变得无限大。
---
###  **前馈网络 (Feedforward Network)**
![[Gemini_Generated_Image_ueftlyueftlyueft (2).png]]
前馈网络，就像我在图中指向的那个大方块，是一个完全连接的神经网络层，用于对输入信息进行更复杂的非线性变换。它通常也包含自己的残差连接和层归一化。在Transformer等现代架构中，它是处理信息的关键部分，负责将不同位置的信息进行整合和变换。