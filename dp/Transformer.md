ResNet-18 VGG-16：数字代表其中一些网络层，也和其中的网络模型层的复用有关，即这些网络都是由一些基本模块不断重复堆叠构建的

网络模型架构图：![[截屏2026-01-14 11.14.15.png]]
N：说明编码器和解码器中的一些网络模型层可能可以不断的迭代
词嵌入向量：计算机识别不了文字，转换成数字向量矩阵
位置编码：自然语言文字有时间顺序，positional encoding 也是一个向量
多头注意力机制：是一个神经网络层
Add：残差连接
Norm：层归一化
前馈神经网络：可以理解为全连接神经网络
Output Embedding：起始符、终止符和中文的词向量
~={green}多头注意力机制=~：带有**因果掩码**的多头注意力机制
~={purple}多头注意力机制=~：输入即来自编码器也来自解码器![[截屏2026-01-14 11.28.14.png]]
PS：模型输入是同时输入 I Love You，输出是有先后顺序的，outputs shifted right 会给一个起始键（“s”），经过中间的神经网络层提取最后得到的是“我”，再循环，将“我”放到下面，输入是“s”和“我”，再经过神经网络层不断的提取，再输出爱到下面，再将“s”和“我”和“爱”三个输入到网络当中，最后输出“你”，再将你放到下面，再重新输入到网络中得到 end，结束。大模型是基于 transformer 架构的，所以都是一个字一个字或者一个词一个词这么输出的 ![[截屏2026-01-14 15.59.55.png]]  
经过全连接层再通过 softmax 输入对应的词或句子的向量，例如“我”，在词表当中是一个位置，结果概率值是 0.95 
🧚‍♀️那这样不就也是依赖前面的结果作为输入了吗？怎么做到并行？❓；大家千万不要搞混“训练阶段”和“推理阶段”
#### 独热
Transformer 的输入是自然语言，即汉语或英语等，模型不能直接识别自然语言，输入到网络模型之前需要将语言转换成数值，模型计算这些数值，其输出的结果对应的就是数值
![[截屏2026-01-14 16.30.27.png]]
Token 就是单独的字或者词
独热编码是最简单的，数值运算和句子有直接的语意对应关系
模型输入：自然语言数据-->词嵌入向量-->与位置编码融合（为了输入的句子即带有语义信息还带有先后顺序信息），输入到模型当中，做训练、推理和验证等工作
#### 词嵌入向量
解决独热向量中纬度高导致模型难以计算的问题
##### 本质 ：构建词嵌入矩阵 $d \times v$ 
词表 ：从数据中获取的，将对应的句子切割成对应的 token 进行去重，$v$ 代表词表当中词的数量（token 的数量）
因为 v 比较大，所以构建词嵌入矩阵的时候 **d 要远小于 v**![[截屏2026-01-14 16.56.34.png]]
相乘只有橙色那一列被保存下来了，这一列代表篮球的独热向量....
从独热向量 $v \times v$ 转换成 $d \times v$ 矩阵
🧚‍♀️降维了，减少参数量
$d$ 可以人为去设置
词嵌入不是随机的，而是由模型训练得到的权重表，词嵌入向量的每一列都是代表一个对应的词（token）的，每一个词又有实际的语义意思，所以经过模型训练就可以将每个列向量映射为一个 token，大致的训练后的结果应该将语义相似的聚类，差异的分散，便于做相似度计算
🧚‍♀️这个和卷积神经网络一个意思，卷积神经网络卷积提取的是特征，这个里是对词向量做提取；相当于把独热向量降维了

降维之后 $v$ 个词大小是 $1 \times d$ ，这 $v$ 个词还可以进行分类，主语、谓语、实际物体（可以进一步分类），可以将 $1 \times d$ 的各种向量降到 $1 \times 2$ 的维度，也就可以在平面中绘制出
词嵌入矩阵功能：
模型表达矩阵维度下降
矩阵数值经过训练，通过数值可以表达对应词的关系
🧚‍♀️乘以独热向量只是让你理解词嵌入矩阵中的每一列都能代替原来的独热向量啊，之前不是说独热向量不好，要改进么。；乘以独热向量可以理解为查表，因为独热向量中只有一个 1 有意义，而查表就是属于“很快”的操作

##### 位置编码
transformer 模型中位置编码是用三角函数构建的
$$\begin{aligned}&PE(pos,2i)=\sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)\quad PE(pos,2i+1)=\cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)\\&\text{其中:}\\&d{:}\text{词嵌入矩阵的大小}\quad i=0,1,\ldots,\frac{d}{2}-1\quad pos：\text{当前token在序列中的第几个位置}\end{aligned}$$
PE 是位置编码的向量数值（是 position embedding 的缩写），是用三角函数构建的，三角函数的数值是由pos、i、d 的这三个参数的数值控制的
一个词嵌入向量的维度大小应该是 $d \times v$ 的大小，$v$ 是整个词表的大小（即多少token 的词），d 是每个token 这样的向量的维度的大小
![[截屏2026-01-19 10.12.33.png|200]]
而词向量信息输到模型中需要配对应的位置编码信息，位置编码也是 $d \times 1$ 的大小以便和词向量相加，i 可以计算对应向量位置编码的数值，奇数位用cos 计算，偶数位用sin 计算
🧚‍♀️为什么喜欢把特征竖着写呢，一般矩阵的行代表样本数，在时序里来代表 len，列来表示特征为度啊；如果计算 pos＝0 的时候咋办？不就变成了 0  1 循环吗？==>是的，第一个位置向量就是零和一的循环‼️；d 是 512   i 从 0-255  m 每个 i 计算一个 sin 一个 cos 值 $2  \times 256=512$  他这里第二个说错了

向量编码实际是固定的，$Input = WE + PE$，语义向量（Word Embedding）是 $WE$，是模型训练出来的。![[截屏2026-01-19 11.07.51.png]]
🧚‍♀️没错，纵轴是文字在句子中的位置，横轴是权重值在词向量中的位置（即 $1 \times$
$512$ 中里面的数排在第几个），颜色表示刚刚经过 sin cos 计算得到的值; 我觉得他这里向量一开始就应该是按照现在的行和列去写，列是维度，行是词矩阵的词语个数

### 模型整体架构
包含编码器和解码器的部分，他们的网络模型层的结构是大差不差的，attention 的算法模型在transformer 当中很重要，用的是多头注意力机制，编码器多头注意力机制是用的是自注意力机制，解码器绿色的也是自注意力机制，但是紫色的不是，但是也差别不大，只不过输入的部分来自于不同的网络层，而绿色和橙色的输入都来自于前一层的输出

自注意力机制的算法模型
1. 计算输入数据x 之间的关联程度