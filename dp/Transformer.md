ResNet-18 VGG-16：数字代表其中一些网络层，也和其中的网络模型层的复用有关，即这些网络都是由一些基本模块不断重复堆叠构建的

网络模型架构图：![[截屏2026-01-14 11.14.15.png]]
N：说明编码器和解码器中的一些网络模型层可能可以不断的迭代
词嵌入向量：计算机识别不了文字，转换成数字向量矩阵
位置编码：自然语言文字有时间顺序，positional encoding 也是一个向量
多头注意力机制：是一个神经网络层
Add：残差连接
Norm：层归一化
前馈神经网络：可以理解为全连接神经网络
Output Embedding：起始符、终止符和中文的词向量
~={green}多头注意力机制=~：带有**因果掩码**的多头注意力机制
~={purple}多头注意力机制=~：输入即来自编码器也来自解码器![[截屏2026-01-14 11.28.14.png]]
PS：模型输入是同时输入 I Love You，输出是有先后顺序的，outputs shifted right 会给一个起始键（“s”），经过中间的神经网络层提取最后得到的是“我”，再循环，将“我”放到下面，输入是“s”和“我”，再经过神经网络层不断的提取，再输出爱到下面，再将“s”和“我”和“爱”三个输入到网络当中，最后输出“你”，再将你放到下面，再重新输入到网络中得到 end，结束。大模型是基于 transformer 架构的，所以都是一个字一个字或者一个词一个词这么输出的   
Q：那这样不就也是依赖前面的结果作为输入了吗？怎么做到并行？