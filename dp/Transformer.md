ResNet-18 VGG-16：数字代表其中一些网络层，也和其中的网络模型层的复用有关，即这些网络都是由一些基本模块不断重复堆叠构建的

网络模型架构图：![[截屏2026-01-14 11.14.15.png]]
N：说明编码器和解码器中的一些网络模型层可能可以不断的迭代
词嵌入向量：计算机识别不了文字，转换成数字向量矩阵
位置编码：自然语言文字有时间顺序，positional encoding 也是一个向量
多头注意力机制：是一个神经网络层
Add：残差连接
Norm：层归一化
前馈神经网络：可以理解为全连接神经网络
Output Embedding：起始符、终止符和中文的词向量
~={green}多头注意力机制=~：带有**因果掩码**的多头注意力机制
~={purple}多头注意力机制=~：输入即来自编码器也来自解码器![[截屏2026-01-14 11.28.14.png]]
PS：模型输入是同时输入 I Love You，输出是有先后顺序的，outputs shifted right 会给一个起始键（“s”），经过中间的神经网络层提取最后得到的是“我”，再循环，将“我”放到下面，输入是“s”和“我”，再经过神经网络层不断的提取，再输出爱到下面，再将“s”和“我”和“爱”三个输入到网络当中，最后输出“你”，再将你放到下面，再重新输入到网络中得到 end，结束。大模型是基于 transformer 架构的，所以都是一个字一个字或者一个词一个词这么输出的 ![[截屏2026-01-14 15.59.55.png]]  
经过全连接层再通过 softmax 输入对应的词或句子的向量，例如“我”，在词表当中是一个位置，结果概率值是 0.95 
🧚‍♀️那这样不就也是依赖前面的结果作为输入了吗？怎么做到并行？❓；大家千万不要搞混“训练阶段”和“推理阶段”
#### 独热
Transformer 的输入是自然语言，即汉语或英语等，模型不能直接识别自然语言，输入到网络模型之前需要将语言转换成数值，模型计算这些数值，其输出的结果对应的就是数值
![[截屏2026-01-14 16.30.27.png]]
Token 就是单独的字或者词
独热编码是最简单的，数值运算和句子有直接的语意对应关系
#### 词嵌入向量
解决独热向量中纬度高导致模型难以计算的问题
##### 本质 ：构建词嵌入矩阵 $d \times v$ 
词表 ：从数据中获取的，将对应的句子切割成对应的 token 进行去重，$v$ 代表词表当中词的数量（token 的数量）
因为 v 比较大，所以构建词嵌入矩阵的时候 **d 要远小于 v**![[截屏2026-01-14 16.56.34.png]]
相乘只有橙色那一列被保存下来了，这一列代表篮球的独热向量....
从独热向量 $v \times v$ 转换成 $d \times v$ 矩阵
🧚‍♀️降维了，减少参数量
$d$ 可以人为去设置
词嵌入不是随机的，而是由模型训练得到的权重表，词嵌入向量的每一列都是代表一个对应的词（token）的，每一个词又有实际的语义意思，所以经过模型训练就可以将每个列向量映射为一个 token，大致的训练后的结果应该将语义相似的聚类，差异的分散，便于做相似度计算
🧚‍♀️这个和卷积神经网络一个意思，卷积神经网络卷积提取的是特征，这个里是对词向量做提取