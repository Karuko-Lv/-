Gradient descent
🧚‍♀️梯度下降就是一个最优化方法吧？ ；是的，梯度下降就是一个**最优化方法**。**反向传播** 负责告诉你“往哪走”（计算出梯度）。**梯度下降** 负责“真正走起来”（利用梯度去更新参数）。一个负责指路，一个负责干活。
Mnist 数字图像的分辨率为 $28 \times 28$ 像素每个像素
随意选取2 层隐藏层每层 16 个神经元，整个网络有 13000 多个权重偏置值可以调整，这些数值决定网络实际做的工作

想要一种算法，给网络看一堆训练数据，希望分层结构可以让模型学会识别训练数据之外的图像
🧚‍♀️有的验证码前一半对就给过同时把后一半拿去做机器学习比如 reCaptcha，！LeCun 是我们 NYU 的教授；广西不是由标记公司嘛本质就是人来做标记给机器学习用

机器学习本质上是一道微积分习题，实际上是找某个函数的最小值，决定神经元激活值的加权和中的权重像是连接的强弱，bias 表明神经元是否更容易被激活
一开始，完全随机出实话所有的权重和偏置值，所以网络对给定的训练实例会表现的很糟糕，所以要定义一个**代价函数**来告诉电脑

🧚‍♀️这个函数有数万个别变量控制