Gradient descent
🧚‍♀️梯度下降就是一个最优化方法吧？ ；是的，梯度下降就是一个**最优化方法**。**反向传播** 负责告诉你“往哪走”（计算出梯度）。**梯度下降** 负责“真正走起来”（利用梯度去更新参数）。一个负责指路，一个负责干活。
Mnist 数字图像的分辨率为 $28 \times 28$ 像素每个像素
随意选取2 层隐藏层每层 16 个神经元，整个网络有 13000 多个权重偏置值可以调整，这些数值决定网络实际做的工作

想要一种算法，给网络看一堆训练数据，希望分层结构可以让模型学会识别训练数据之外的图像
🧚‍♀️有的验证码前一半对就给过同时把后一半拿去做机器学习比如 reCaptcha，！LeCun 是我们 NYU 的教授；广西不是由标记公司嘛本质就是人来做标记给机器学习用

机器学习本质上是一道微积分习题，实际上是找某个函数的最小值，决定神经元激活值的加权和中的权重像是连接的强弱，bias 表明神经元是否更容易被激活![[截屏2026-01-20 14.04.54.png]]
一开始，完全随机出实话所有的权重和偏置值，所以网络对给定的训练实例会表现的很糟糕，所以要定义一个**代价函数**来告诉电脑，当网络对图像能进行正确分类时这个平方和就比较小，用这个平均代价评价网络是好的还是坏的![[截屏2026-01-20 14.10.05.png]]
![[截屏2026-01-20 14.09.45.png]]
由于对 13000 多元的代价函数基本不能简单通过求导等于 0 来求得最值了，所以先随便挑一个输入值，考虑哪个方向走函数值会变小，斜率正往左、负往右，每个点都这样重复、计算新斜率、适当走一小步会逼近函数的某个局部最小值
但是即使一元函数，不知道一开始输入值在哪里，最后可能会落到许多不同的坑里，且无法保证落到的局部最小值就是代价函数可能达到的全局最小值，如果每步的大小和斜率成正比，那么最小值附近斜率会越来越平缓，每步会越来越小，这样可以防止调过头
函数的提督指出了函数的最陡增长方向==>按反梯度反向函数值降低的就最快。梯度向量的长度代表最陡的斜坡到底有多陡
![[截屏2026-01-20 14.45.48.png]]
![[截屏2026-01-20 14.49.04.png]]
负梯度指出在大到爆炸的函数的输入空间内具体如何改变每一项的参数才可以让代价函数的值下降得最快 
代价函数取整个训练集的平均值，所以最小化即对所有的样本得到的总体结果都更好一点
**网络学习的本质**：让代价函数的值最小==>为了达到这个结果，代价函数非常有必要是平滑的才能够每次挪一点点最后找到一个局部最小值，这也是人工神经元激活值是连续的原因，而非直接沿袭生物学神经元那种二元式的，要么激活要么非激活的取值模式 
梯度下降是一种可以收敛到代价函数图中某一个坑里的（局部最小值）


🧚‍♀️这个函数有数万个别变量控制；其实带一点搜索的味道；平方残差和？；有点像二阶中心矩；反应数据偏离的大小；输出 0-1 之间的概率才是 softmax。这里不应该是求方差，而是要求交叉熵损失；这不是方差；一万三千变量求导......；非凸函数会有不同的最优解；一元函数的梯度下降法，和高中数学导数题折腾的东西类似；陷入局部最优；最优化理论，梯度下降；我现在唯一没有想通的是这个斜率（梯度）是怎么算出来的；这就是个优化问题，模拟退火只是其中一个算法。。。；没关系啊，训练样本够大就能从局部解里找到最优，遗传退火太费时间了；直接找 k 值为零不行么，是因为难计算还是我的想法有误。。。==>- $$Loss = (ReLU(w_2 \cdot ReLU(w_1 \cdot x)) - y)^2$$当对这个式子求导并令其等于 0 时，你会得到一个超越方程 (Transcendental Equation)。
在数学上是无法把 $w$ 单独放在等号一边的。也就是说，这个方程组根本没有代数公式解。你算不出来那个完美的 $w$ 是多少；learning-rate 學習率；撒下多个点是遗传算法；⬅️ 往往采取 batch + dropout 的方法防止过拟合的陷入局部最优的情况出现；然后每得到一组 w 列向量，就要代入运行上万次来求出“代价”值？；先得到代价函数的表达式提前手动求梯度表达式之后再代进当前点，求出当前点的梯度吗？；不是的，前面说了，代价函数是对所有样本的求和平均，先通过所有样本构建一个代价函数，再通过数学推导求偏导函数，最后随机选一组加权进行迭代；平滑-处处可微；哈哈哈, 生物神经元是上亿个只会取 01 的节点, 电脑哪儿搞这么多变量去；神 TM 穷举，这叫迭代；表示一个方法不知可行，把函数模型限制在某个规则区域里里，然后按照梯度下降求出极点，这样就可以掌握计算区域的波动情况（类似求导函数，反推函数的信息）
![[截屏2026-01-20 15.04.28.png]] 梯度标记出了改变哪个参数性价比最高
![[截屏2026-01-20 15.12.56.png]]中间的隐藏层感觉就像在深海一般 13000 维的参数空间中网络找一个还不错的局部最小值住下来
🧚‍♀️这里用了贪心策略，或许使用 dp 能提高数据利用率？；个人理解其实是记住了数据集结构如果其中手写字体不在中心且存在大量空白这样的样本估计会增加错误率；隐含层的意义是处理线性不可分的情况，因为函数是线性的一旦遇到非线性情况就需要隐含层；哈哈用googleNET 试试嘛；隐含层提取高维特征，解决线性不可分问题；感觉每一张图各自倾向于一部分信息，比如更边缘、更中心等等；←在图像生成中，咱们确实可以看出，教会机器怎么在画里写字是一个技术难题


🤔❓思考如果要让机器更好的识别出短边和图案，应该如何读取图像？![[截屏2026-01-20 15.20.31.png]]
代价函数最小到底是对应上了图像中的某些结构还是只是单纯的死记硬背，记住了整个正确分类的数据集，如果训练用了一个随机的训练集代价下降的就会特别慢，和一个线性方程差不多，很难找到可能存在的局部最小值，即很难找到让准确率上升的权重，但是如果用结构化的训练集，拥有标记正确的数据，一开始代价值会上下浮动，后来降到很低地方，达到了一定准确率，所以某种程度上更容易找到局部最小值，如果数据最小化就能更轻松的找到局部最小
🧚‍♀️之前有 paper 指出 resnet 的数学机制和 bootstrap 相似，其实是在投票；arxiv. Org/abs/1706.05394；细思极恐，这种我们没发现的“智能”不知道怎么用数学解释？；可能因为图像和答案之间有逻辑关联比如形状和语言，错误的没有；所以要利用数据蒸馏技术；意思是我们通过优化数据集可以显著降低训练难度，方便引导模型参数的收敛？
**![[Pasted image 20260120154345.png]]**
横坐标：**梯度下降的步数**（或者叫**迭代次数**）。
**它的具体含义是：**
- 它代表了神经网络被**训练了多少轮**。
- 每往右走一步（Step），就意味着模型做了一次“计算梯度 -> 更新权重”的操作。
- 随着步数增加（向右移动），你可以看到纵坐标的 **Cost function**（代价/误差）正在逐渐下降，这表示模型正在通过不断的训练变得越来越准确。
### 梯度

#### 1. 简单的回答
- **梯度的方向**：指出了函数值**增长最快**（最陡峭）的方向。
- **梯度的模（长度）**：就是这个“最快增长方向”上的**变化率**（也就是**最大的方向导数**）。
#### 2. 什么是梯度？（直观想象）
想象你在爬一座形状复杂的山（函数面 $z = f(x, y)$）。你站在半山腰的某一个点上。
此时，你可以往东南西北 360 度任意一个方向迈一步：
- 往**东**走，可能是平路（方向导数 = 0）。
- 往**西**走，可能是下坡（方向导数 < 0）。
- 往**东北**走，可能是上坡（方向导数 > 0）。
在所有可能的方向中，一定有一个方向是**最陡峭的、让你上升得最快的**。
- **梯度向量（Vector）：** 就是地上画的一个箭头，指向这个**最陡峭的上坡方向**。
- **梯度的模（Scalar）：** 就是如果你顺着这个箭头走，坡度到底有**多陡**（比如坡度是 60度 还是 10度）。
#### 3. 为什么“模 = 最大方向导数”？（简单的数学证明）

这是一个非常漂亮的数学推导，用向量的**点积 (Dot Product)** 一眼就能看出来。
假设：

- $\nabla f$ 是梯度向量。$$\nabla f(x, y) = \left( \frac{\partial f}{\partial x}, \ \frac{\partial f}{\partial y} \right)= \begin{bmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{bmatrix}$$
- $\vec{u}$ 是一个单位向量（代表你想要走的方向）。
- $\theta$ 是梯度和你走的方向之间的夹角。

方向导数 ($D_u f$) 的公式是：
$$D_u f = \nabla f \cdot \vec{u} = |\nabla f| \cdot |\vec{u}| \cdot \cos(\theta)$$

因为 $\vec{u}$ 是单位向量，长度 $|\vec{u}| = 1$，所以公式简化为：

$$D_u f = |\nabla f| \cdot \cos(\theta)$$

**现在我们要找最大值：**

- $|\nabla f|$ 是个固定值（这一点的梯度长度是固定的）。
- 我们要让 $D_u f$ 最大，其实就是让 $\cos(\theta)$ 最大。
- $\cos(\theta)$ 的最大值是 **1**，此时 $\theta = 0$度。
**结论：**
1. **方向：** 当 $\theta = 0$ 时，也就是**你走的方向 ($\vec{u}$) 和梯度 ($\nabla f$) 完全重合时**，增长最快。
2. **数值：** 此时的最大值就是 $|\nabla f| \cdot 1 = |\nabla f|$。也就是**梯度的模**。
#### 4. 在深度学习中的反直觉点
虽然梯度的定义是**“增长最快”**的方向，但在深度学习（训练模型）里，我们用的是 **梯度下降 (Gradient Descent)**。
因为我们的目标是让 Loss（误差）**变小**，也就是要**下山**，而不是上山。
所以公式里永远带着一个负号：

$$w_{new} = w_{old} \mathbf{-} \eta \cdot \nabla Loss$$

- **$\nabla Loss$ (梯度)**：告诉我往哪走 Loss 会**暴涨**。
- **$-$ (负号)**：好的，那我**掉头**往反方向走（负梯度方向），这样 Loss 就会**暴跌**（下降最快）。