Gradient descent
🧚‍♀️梯度下降就是一个最优化方法吧？ ；是的，梯度下降就是一个**最优化方法**。**反向传播** 负责告诉你“往哪走”（计算出梯度）。**梯度下降** 负责“真正走起来”（利用梯度去更新参数）。一个负责指路，一个负责干活。
Mnist 数字图像的分辨率为 $28 \times 28$ 像素每个像素
随意选取2 层隐藏层每层 16 个神经元，整个网络有 13000 多个权重偏置值可以调整，这些数值决定网络实际做的工作

想要一种算法，给网络看一堆训练数据，希望分层结构可以让模型学会识别训练数据之外的图像
🧚‍♀️有的验证码前一半对就给过同时把后一半拿去做机器学习比如 reCaptcha，！LeCun 是我们 NYU 的教授；广西不是由标记公司嘛本质就是人来做标记给机器学习用

机器学习本质上是一道微积分习题，实际上是找某个函数的最小值，决定神经元激活值的加权和中的权重像是连接的强弱，bias 表明神经元是否更容易被激活![[截屏2026-01-20 14.04.54.png]]
一开始，完全随机出实话所有的权重和偏置值，所以网络对给定的训练实例会表现的很糟糕，所以要定义一个**代价函数**来告诉电脑，当网络对图像能进行正确分类时这个平方和就比较小，用这个平均代价评价网络是好的还是坏的![[截屏2026-01-20 14.10.05.png]]
![[截屏2026-01-20 14.09.45.png]]
由于对 13000 多个

🧚‍♀️这个函数有数万个别变量控制；其实带一点搜索的味道；平方残差和？；有点像二阶中心矩；反应数据偏离的大小；输出 0-1 之间的概率才是 softmax。这里不应该是求方差，而是要求交叉熵损失；这不是方差；一万三千变量求导......