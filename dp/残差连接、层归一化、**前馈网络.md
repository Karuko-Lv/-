### **残差连接 (Residual Connection)**
![[Gemini_Generated_Image_ueftlyueftlyueft.png]]
就像在图上指的一样，残差连接就像一条“捷径”。它允许信息绕过一些层，直接传递到后面的层。这有助于解决深层网络训练中的“梯度消失”问题，让网络更容易学习到复杂的特征
~={red}==>原理：公式 $x + f(x)$ =~
### **层归一化 (Layer Normalization)**
![[Gemini_Generated_Image_ueftlyueftlyueft (1).png]]
如上图所示，**层归一化通常紧跟在残差连接之后**。它的作用就像一个“整理器”，对这一层所有神经元的输出进行标准化，使它们的分布更加稳定。这有助于加速训练过程，并提高模型的泛化能力。

###  **前馈网络 (Feedforward Network)**
![[Gemini_Generated_Image_ueftlyueftlyueft (2).png]]
前馈网络，就像我在图中指向的那个大方块，是一个完全连接的神经网络层，用于对输入信息进行更复杂的非线性变换。它通常也包含自己的残差连接和层归一化。在Transformer等现代架构中，它是处理信息的关键部分，负责将不同位置的信息进行整合和变换。