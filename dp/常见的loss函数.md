在深度学习和机器学习中，Loss 函数（损失函数）种类繁多，但我们不需要全部背下来。

我们只需要根据 **“你要解决什么任务”** 来把它们分为三大门派：**回归派**、**分类派** 和 **特殊派**。

以下是极其精华的分类清单：

---

### 第一大门派：回归损失 (Regression Loss)

适用场景： 预测连续的数值（如房价、气温、坐标）。

核心思想： 衡量“预测值”和“真实值”在这个数字轴上差多远。

#### 1. 均方误差 (MSE - Mean Squared Error) / L2 Loss

- **地位：** **最常用**，默认选项。
    
- **公式：** $(y - \hat{y})^2$
    
- **特点：**
    
    - 因为有平方，它对**异常值 (Outliers)** 非常敏感。
        
    - 如果你预测错了一点点，它惩罚很轻；如果你错得很离谱，它会给你**平方级**的重罚。
        
- **缺点：** 如果数据里脏数据（噪点）很多，MSE 会被这些噪点带偏。
    

#### 2. 平均绝对误差 (MAE - Mean Absolute Error) / L1 Loss

- **公式：** $|y - \hat{y}|$
    
- **特点：**
    
    - **铁面无私**。不管你错多少，我都按线性比例惩罚。
        
    - **鲁棒性强：** 它不在乎异常值，不容易被噪点带偏。
        
- **缺点：** 在 0 点处不可导（折线尖角），梯度下降到最后时可能来回震荡，不容易收敛到绝对精确的 0。
    

#### 3. Huber Loss (平滑 L1 损失)

- **地位：** 上面两两者的**混血儿**（结合了优点）。
    
- **特点：**
    
    - 当误差很小时：它变身成 MSE（保证光滑可导，收敛快）。
        
    - 当误差很大时：它变身成 MAE（减少对异常值的敏感）。
        
- **应用：** 目标检测（如 YOLO, R-CNN）回归边框坐标时常用。
    

---

### 第二大门派：分类损失 (Classification Loss)

适用场景： 预测类别、概率（如猫/狗、手写数字、推荐系统）。

核心思想： 衡量分布的差异，或者叫“惊吓程度”。

#### 1. 二元交叉熵 (Binary Cross-Entropy / Log Loss)

- **适用：** **二分类**问题（Yes/No）。
    
- **搭配：** 输出层通常用 **Sigmoid** 激活函数。
    
- **特点：** 专门处理 0 和 1 的概率。
    

#### 2. 类别交叉熵 (Categorical Cross-Entropy)

- **适用：** **多分类**问题（猫/狗/猪）。
    
- **要求：** 标签必须是 **One-hot 编码**（如 `[0, 1, 0]`）。
    
- **搭配：** 输出层通常用 **Softmax** 激活函数。
    

#### 3. 稀疏类别交叉熵 (Sparse Categorical Cross-Entropy)

- **适用：** 也是多分类。
    
- **区别：** 仅仅是**数据格式**不同。如果你的标签是**整数**（如 `Cat=1`），不想转成 One-hot 浪费内存，就用这个。数学原理和上面那个完全一样。
    

#### 4. Hinge Loss (合页损失)

- **来源：** **SVM (支持向量机)** 的标配。
    
- **特点：** 它不关心概率，只关心**“边界”**。它要求分类不仅要对，还要把两类分得越开越好（最大化间隔）。
    
- **应用：** 有时候在 GAN（生成对抗网络）或者某些特定分类任务中会用到。
    

---

### 第三大门派：特殊/高级损失 (Advanced)

**适用场景：** 生成模型、概率分布、图像分割等。

#### 1. KL 散度 (Kullback-Leibler Divergence)

- **别名：** 相对熵。
    
- **含义：** 衡量**两个概率分布长得像不像**。
    
- **应用：** **VAE (变分自编码器)**、强化学习 (PPO)。它不是为了比对“对错”，而是为了让模型学出的分布去拟合正态分布。
    

#### 2. Triplet Loss (三元组损失)

- **应用：** **人脸识别**。
    
- **逻辑：** 给我三张图（Anchor, Positive, Negative）。
    
- **目的：** 让“我”和“我的照片”距离拉近，让“我”和“别人的照片”距离拉远。
    

#### 3. IoU Loss / Dice Loss

- **应用：** **图像分割**（医学影像、自动驾驶车道线）。
    
- **逻辑：** 衡量预测的“区域形状”和真实形状的**重叠程度**。
    

---

### 总结：我该选哪一个？（作弊表）

|**你的任务**|**你的标签格式**|**首选 Loss 函数**|
|---|---|---|
|**预测房价/气温**|连续数字|**MSE (均方误差)**|
|**预测异常值多的数据**|连续数字|**MAE 或 Huber**|
|**二分类 (是/否)**|0 或 1|**Binary Cross-Entropy**|
|**多分类 (N选1)**|One-hot `[0,1,0]`|**Categorical Cross-Entropy**|
|**多分类 (N选1)**|整数 `2`|**Sparse Categorical Cross-Entropy**|
|**人脸识别**|同一个人|**Triplet Loss**|
|**分布拟合**|概率分布|**KL Divergence**|